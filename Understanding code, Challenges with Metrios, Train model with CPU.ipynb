{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51c16612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c70d4f",
   "metadata": {},
   "source": [
    "# When there is no missing values in the input time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8897f81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assume we input a time series, [batchSize= 1, column number = 1, length of the series = 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0553, -0.0302, -1.9991, -0.1097, -0.7812]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 5)\n",
    "print(\"Assume we input a time series, [batchSize= 1, column number = 1, length of the series = 5]\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d2f9e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chomp1D(x, chomp_size):\n",
    "    return x[:, :, :-chomp_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443b0750",
   "metadata": {},
   "source": [
    "Note: since weightnorm and relu activation function do not change shape, we omit them in this testing code for better clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94f24a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are several Conv1 layers in the whole network. All of the conv1's out_channels=40 except the last conv1's out_channel=reduced_size=160.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Conv1d(1, 160, kernel_size=(3,), stride=(1,), padding=(2,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"There are several Conv1 layers in the whole network. All of the conv1's out_channels=40 except the last conv1's out_channel=reduced_size=160.\")\n",
    "\n",
    "conv1 = torch.nn.Conv1d(in_channels=1, out_channels=160, kernel_size=3, padding=2, dilation=1)\n",
    "conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbea14e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 160, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5028,  0.4929,  0.1409,  ...,  1.3231,  0.4914,  0.8704],\n",
       "         [ 0.0288,  0.0517, -0.2113,  ...,  0.5045, -0.3874,  0.2663],\n",
       "         [-0.4488, -0.5000, -1.5966,  ..., -1.6309, -0.4846, -0.7582],\n",
       "         ...,\n",
       "         [ 0.1665,  0.1464,  0.3184,  ..., -0.3751,  0.4891, -0.0805],\n",
       "         [-0.0182, -0.0467,  0.0271,  ...,  0.2835,  0.4184,  0.0718],\n",
       "         [ 0.0733,  0.0767, -0.5916,  ..., -0.1320, -0.3647,  0.1018]]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_conv1 = conv1(x)\n",
    "print(output_conv1.shape)\n",
    "output_conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "164f1dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 160, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5.0282e-01,  4.9286e-01,  1.4086e-01,  3.4896e-01,  1.3231e+00],\n",
       "         [ 2.8781e-02,  5.1713e-02, -2.1128e-01, -1.1166e+00,  5.0452e-01],\n",
       "         [-4.4884e-01, -4.9997e-01, -1.5966e+00, -4.6470e-01, -1.6309e+00],\n",
       "         [ 1.0023e-01,  7.3756e-02, -6.2157e-01, -1.0542e-01, -5.7074e-01],\n",
       "         [ 5.4731e-02,  7.5744e-02,  1.2276e+00,  1.0719e+00, -4.5056e-01],\n",
       "         [-3.8510e-01, -3.6379e-01, -3.7000e-01, -1.1432e+00, -9.2701e-01],\n",
       "         [-3.0202e-01, -3.4635e-01, -1.3438e+00, -3.5723e-01, -8.1518e-01],\n",
       "         [ 5.7209e-02,  2.2417e-02, -4.1164e-01,  6.2231e-01,  1.0079e+00],\n",
       "         [-3.2787e-01, -2.8782e-01,  1.6315e-01, -1.0393e+00, -1.1890e+00],\n",
       "         [ 3.1724e-01,  3.1578e-01,  9.6562e-02,  1.4263e-03, -2.3204e-01],\n",
       "         [ 1.7586e-01,  1.9580e-01,  6.0805e-01,  1.7057e-01,  8.4925e-01],\n",
       "         [ 4.5029e-01,  4.1284e-01, -2.1712e-01,  7.5297e-01,  4.6609e-01],\n",
       "         [ 3.0861e-01,  2.9685e-01, -6.7011e-01, -8.2267e-01, -6.5493e-01],\n",
       "         [-5.3144e-01, -4.9377e-01,  5.8379e-01, -1.6866e-01, -6.9127e-01],\n",
       "         [-6.9571e-03, -3.4439e-02, -2.9067e-01,  5.1637e-01, -2.1624e-01],\n",
       "         [ 5.5774e-01,  5.6929e-01,  1.0759e+00,  9.3525e-01,  4.7206e-01],\n",
       "         [ 3.7763e-01,  4.1090e-01,  1.1283e+00,  3.6512e-01,  6.4930e-01],\n",
       "         [ 4.0624e-02,  5.5952e-02,  3.2698e-01, -3.6159e-02,  5.0694e-01],\n",
       "         [-4.5899e-01, -4.8402e-01, -3.6572e-01,  5.6671e-01, -4.0615e-01],\n",
       "         [ 4.1076e-01,  4.5118e-01,  1.5377e+00,  6.6689e-01, -2.3551e-01],\n",
       "         [-5.1240e-01, -5.4537e-01, -1.4312e+00, -7.5155e-01, -5.2053e-01],\n",
       "         [ 7.3775e-02,  4.4874e-02, -4.1796e-01,  3.4736e-01,  1.6197e-01],\n",
       "         [ 3.2741e-01,  3.2545e-01,  5.0447e-01,  6.5734e-01,  1.7399e-01],\n",
       "         [ 1.7442e-01,  1.9630e-01,  9.6566e-01,  5.8021e-01, -3.2447e-01],\n",
       "         [-2.6828e-01, -2.3709e-01,  7.1677e-02, -7.8851e-01,  7.7893e-01],\n",
       "         [-4.2019e-01, -4.3499e-01, -2.9305e-01,  3.5705e-01,  6.7693e-01],\n",
       "         [ 2.8847e-01,  2.5380e-01,  1.8606e-01,  1.3309e+00, -8.9225e-02],\n",
       "         [-2.2266e-01, -2.6435e-01, -9.1576e-01,  1.2305e-01, -1.3789e+00],\n",
       "         [ 4.7639e-01,  4.4028e-01, -1.7301e-01,  8.0233e-01,  1.2434e+00],\n",
       "         [ 3.3422e-01,  3.3778e-01,  2.0522e-01, -4.5969e-02, -6.8904e-01],\n",
       "         [ 2.5986e-01,  2.6173e-01,  9.6609e-01,  1.2515e+00, -5.4255e-02],\n",
       "         [ 3.1384e-01,  3.1755e-01,  4.1647e-01,  4.0197e-01,  1.4095e+00],\n",
       "         [ 9.7630e-02,  1.6450e-01,  1.2538e+00, -4.7328e-01,  4.8632e-01],\n",
       "         [ 2.6653e-01,  3.0466e-01,  6.1871e-01, -4.8580e-01,  1.1985e+00],\n",
       "         [-1.9152e-01, -1.9165e-01,  9.4684e-02,  2.9139e-01,  5.4820e-01],\n",
       "         [-1.7366e-01, -1.7723e-01, -3.6853e-01, -3.5692e-01, -3.8917e-01],\n",
       "         [ 1.4232e-01,  1.7718e-01,  3.1199e-01, -8.4116e-01, -1.1611e-01],\n",
       "         [ 2.8002e-01,  2.7239e-01,  6.6295e-01,  1.1358e+00,  3.5459e-01],\n",
       "         [-1.5934e-01, -1.2069e-01,  4.3899e-01, -6.4317e-01, -7.6549e-01],\n",
       "         [-1.8163e-02,  9.2640e-03,  1.9654e-01, -6.9212e-01, -6.3870e-01],\n",
       "         [-5.4368e-02, -5.0776e-02,  7.9187e-01,  1.0701e+00, -7.0846e-01],\n",
       "         [ 1.3168e-01,  6.4673e-02, -9.7328e-01,  7.3191e-01, -1.1914e+00],\n",
       "         [-5.4104e-01, -5.5968e-01, -5.7556e-01,  5.4022e-02, -6.8603e-01],\n",
       "         [ 4.1250e-02,  7.5879e-02,  6.1040e-01, -2.5336e-01,  1.0481e+00],\n",
       "         [ 1.0490e-02, -3.5288e-02, -9.4043e-01,  9.1148e-02, -1.4353e+00],\n",
       "         [-3.0090e-01, -3.2323e-01, -1.3555e+00, -1.0841e+00,  3.3870e-01],\n",
       "         [ 3.3407e-01,  3.2604e-01, -1.0930e-02,  5.2711e-02, -3.4196e-01],\n",
       "         [-5.7455e-01, -5.5730e-01, -2.6133e-01, -6.8611e-01, -2.4818e-01],\n",
       "         [ 1.1999e-01,  7.6895e-02, -1.0328e+00, -1.5776e-01, -5.7135e-01],\n",
       "         [-3.8883e-01, -4.1544e-01, -3.4882e-01,  6.0398e-01, -4.6799e-01],\n",
       "         [-2.9612e-01, -3.1382e-01, -7.6689e-01, -3.5253e-01,  3.5748e-01],\n",
       "         [-4.6277e-01, -4.6860e-01,  1.3909e-01,  6.9825e-01,  2.2165e-01],\n",
       "         [-3.1361e-01, -2.9813e-01, -4.3475e-02, -4.1722e-01,  2.0794e-01],\n",
       "         [-3.7246e-01, -3.8082e-01, -3.4429e-02,  4.0316e-01, -9.5035e-01],\n",
       "         [ 3.4389e-01,  2.8620e-01, -2.9689e-01,  1.3808e+00,  2.9409e-02],\n",
       "         [-1.3467e-01, -1.2148e-01,  3.8279e-01,  1.4845e-01, -8.3726e-01],\n",
       "         [ 5.9382e-02,  8.2828e-02,  9.0245e-02, -7.5777e-01, -6.2166e-01],\n",
       "         [ 2.6447e-02,  6.5462e-02,  6.5079e-01, -3.1675e-01,  1.4139e+00],\n",
       "         [ 1.2977e-01,  1.2246e-01, -1.3230e-01, -6.3454e-02, -7.7280e-01],\n",
       "         [-6.4993e-02, -8.6333e-02,  4.5499e-02,  8.6924e-01,  1.9589e-01],\n",
       "         [-5.2518e-01, -5.1298e-01, -5.9166e-01, -1.0033e+00,  3.9296e-01],\n",
       "         [ 8.2535e-02,  1.0271e-01,  1.2059e+00,  1.0761e+00, -3.2311e-02],\n",
       "         [-4.9535e-01, -5.1892e-01, -1.0573e+00, -4.8393e-01,  1.5546e-01],\n",
       "         [-2.3374e-01, -2.5485e-01, -1.3230e+00, -1.1601e+00, -4.2500e-01],\n",
       "         [ 4.6585e-01,  4.3266e-01, -3.8276e-01,  3.0333e-01, -2.0672e-01],\n",
       "         [-5.1612e-01, -4.8031e-01, -3.1459e-01, -1.4106e+00,  5.1069e-01],\n",
       "         [-2.3099e-01, -2.4022e-01, -1.0537e+00, -1.1592e+00, -2.4444e-01],\n",
       "         [-2.4107e-02, -2.3723e-02, -2.0221e-01, -3.1681e-01, -1.6399e-01],\n",
       "         [-2.9755e-01, -3.2219e-01, -6.7002e-01, -6.2958e-02, -1.4419e+00],\n",
       "         [-3.4826e-01, -3.0968e-01, -8.7918e-02, -1.3036e+00, -2.4724e-01],\n",
       "         [-5.6333e-01, -5.6585e-01, -4.2645e-01, -2.7165e-01, -6.5484e-01],\n",
       "         [-3.9300e-01, -3.7437e-01,  4.2324e-01,  2.1948e-01,  5.1990e-02],\n",
       "         [-5.6667e-01, -5.5947e-01,  3.7224e-01,  5.6855e-01, -1.2826e+00],\n",
       "         [ 3.4450e-01,  3.6896e-01,  1.3301e+00,  1.0164e+00,  9.0365e-01],\n",
       "         [-4.7126e-01, -5.0223e-01, -9.2405e-01, -1.3631e-01, -1.6205e+00],\n",
       "         [ 3.4566e-01,  3.0643e-01, -2.7562e-02,  1.1226e+00, -2.7554e-01],\n",
       "         [-5.4399e-01, -5.4036e-01,  1.8307e-01,  3.9046e-01, -1.3242e+00],\n",
       "         [-1.0846e-01, -7.1065e-02,  4.3338e-01, -5.4552e-01,  8.3599e-01],\n",
       "         [-4.6158e-01, -3.9041e-01,  6.2189e-01, -1.2719e+00,  3.2431e-01],\n",
       "         [ 4.7733e-01,  4.7223e-01,  2.1952e-01,  2.7273e-01,  6.2788e-01],\n",
       "         [ 5.2970e-01,  5.4077e-01,  1.7900e-01, -3.9162e-01,  5.6605e-01],\n",
       "         [-2.9037e-01, -2.8314e-01,  1.5702e-01,  1.0465e-01, -8.4281e-01],\n",
       "         [ 5.2347e-02,  6.8642e-02,  9.5823e-02, -4.5375e-01,  7.4046e-02],\n",
       "         [ 8.5577e-02,  1.1700e-01,  7.6204e-01, -3.0734e-02, -6.1614e-01],\n",
       "         [ 4.7539e-01,  5.3293e-01,  1.3436e+00, -2.1341e-01,  7.2634e-01],\n",
       "         [ 2.3095e-01,  1.8892e-01, -4.3817e-01,  7.1494e-01,  6.4275e-01],\n",
       "         [ 5.1428e-01,  4.8523e-01,  4.7549e-01,  1.4459e+00, -4.1798e-02],\n",
       "         [-1.8583e-02,  2.1385e-02,  5.3925e-01, -5.0114e-01,  1.2893e+00],\n",
       "         [ 2.2383e-01,  1.7699e-01, -6.4880e-01,  4.6325e-01, -1.1892e+00],\n",
       "         [-1.7179e-01, -1.7300e-01, -5.2838e-01, -6.4603e-01,  2.7995e-01],\n",
       "         [-1.3251e-01, -1.4428e-01, -4.7001e-01, -2.7730e-01, -9.2339e-01],\n",
       "         [ 1.5008e-01,  2.1205e-01,  1.3202e+00, -2.3876e-01,  3.3970e-01],\n",
       "         [ 5.0678e-01,  4.8636e-01,  3.0433e-01,  9.5788e-01,  1.2172e+00],\n",
       "         [-1.6303e-01, -2.3739e-01, -1.2265e+00,  8.2153e-01, -4.0011e-01],\n",
       "         [ 3.5668e-02,  3.0966e-02, -3.7064e-01, -4.8267e-01, -1.1041e+00],\n",
       "         [ 3.0096e-01,  2.6183e-01,  4.0413e-02,  1.2486e+00, -2.7294e-01],\n",
       "         [-4.3430e-01, -4.0213e-01,  5.5231e-01, -5.7146e-02, -3.0670e-01],\n",
       "         [-4.3265e-02, -8.6276e-02, -9.5615e-01,  1.1503e-01,  5.5628e-01],\n",
       "         [ 2.4166e-01,  2.7462e-01,  1.1180e+00,  4.0481e-01,  5.0873e-02],\n",
       "         [-2.0662e-01, -2.2528e-01, -8.2074e-02,  6.3925e-01, -2.1473e-01],\n",
       "         [-3.4767e-01, -3.3028e-01,  1.9193e-01, -1.2204e-01, -6.6370e-02],\n",
       "         [-3.5535e-01, -3.4081e-01,  1.6245e-01, -9.5110e-02, -6.4385e-01],\n",
       "         [ 5.3883e-01,  5.2280e-01,  8.9804e-01,  1.6262e+00,  1.2951e-01],\n",
       "         [ 1.9069e-01,  2.3969e-01,  6.0517e-01, -9.2498e-01, -1.9604e-01],\n",
       "         [ 5.2873e-01,  5.1740e-01,  9.5001e-01,  1.5951e+00,  9.8766e-01],\n",
       "         [-3.3579e-01, -3.4050e-01, -7.4636e-01, -8.2992e-01, -9.3978e-01],\n",
       "         [-1.2147e-01, -1.1613e-01,  5.8500e-01,  8.4826e-01,  1.3122e+00],\n",
       "         [-7.9112e-03, -2.2402e-02, -1.0916e+00, -1.1858e+00, -6.7268e-01],\n",
       "         [-3.6679e-01, -4.0586e-01, -9.5390e-01,  1.4197e-01,  1.1601e-01],\n",
       "         [-3.1314e-01, -2.7778e-01,  7.2117e-01,  5.3427e-02,  3.2137e-01],\n",
       "         [ 2.8163e-01,  2.7193e-01,  5.2308e-01,  1.0007e+00,  4.5958e-01],\n",
       "         [-4.9225e-01, -4.5802e-01, -2.6553e-02, -9.6330e-01, -6.1282e-02],\n",
       "         [-8.4860e-02, -7.1542e-02,  7.7968e-01,  8.1343e-01,  8.0274e-01],\n",
       "         [-4.7408e-02, -6.1142e-02, -8.9606e-02,  3.2889e-01, -7.9548e-01],\n",
       "         [-9.9151e-02, -1.0075e-01,  3.8536e-01,  7.0909e-01,  1.4956e-01],\n",
       "         [ 2.9480e-01,  2.9662e-01, -2.1787e-01, -5.8121e-01, -2.5647e-01],\n",
       "         [ 4.6759e-01,  4.4043e-01, -6.7629e-01, -3.7340e-01, -5.0032e-01],\n",
       "         [-5.1725e-01, -4.8097e-01,  7.0886e-02, -8.4823e-01,  3.6784e-01],\n",
       "         [ 4.9698e-03,  9.0303e-03,  7.2791e-01,  1.0244e+00,  1.0835e+00],\n",
       "         [-1.4541e-01, -1.3306e-01,  6.2250e-02, -2.2205e-01,  5.8612e-01],\n",
       "         [-1.9992e-01, -2.1651e-01, -8.0055e-01, -5.9498e-01, -1.3260e+00],\n",
       "         [ 5.8974e-01,  5.6189e-01, -3.5103e-02,  5.9955e-01,  2.5595e-01],\n",
       "         [ 3.1494e-01,  3.0979e-01,  8.1437e-01,  1.3344e+00,  1.6760e+00],\n",
       "         [ 4.0958e-01,  3.9433e-01,  2.7140e-01,  7.0596e-01, -1.1972e-01],\n",
       "         [ 3.7596e-01,  3.6115e-01, -1.4108e-01,  1.0709e-01,  3.2526e-01],\n",
       "         [ 4.2780e-01,  3.7846e-01, -8.6966e-02,  1.4104e+00,  9.4646e-01],\n",
       "         [-8.7855e-02, -5.0954e-02,  6.4864e-01, -3.0754e-01, -8.2576e-01],\n",
       "         [ 1.0464e-01,  1.6426e-01,  1.0809e+00, -4.8811e-01,  4.4520e-01],\n",
       "         [ 1.2491e-02,  1.8487e-02, -5.9635e-01, -1.1876e+00, -1.0733e+00],\n",
       "         [ 1.4740e-04,  1.0349e-02,  1.2996e-01, -1.0629e-01,  9.6085e-01],\n",
       "         [-4.4985e-01, -4.2720e-01, -4.3195e-01, -1.2016e+00, -1.3690e-01],\n",
       "         [-2.8473e-01, -3.1927e-01, -1.2578e+00, -5.4479e-01, -1.8363e-01],\n",
       "         [ 4.1344e-02, -2.0527e-02, -1.0020e+00,  5.9508e-01, -5.7520e-01],\n",
       "         [ 3.2990e-01,  2.9137e-01,  1.5015e-01,  1.4185e+00,  4.3882e-01],\n",
       "         [-4.0567e-01, -3.9807e-01, -8.1447e-02, -2.2891e-01, -1.2655e+00],\n",
       "         [ 5.5143e-01,  5.6879e-01,  1.1880e+00,  9.6518e-01,  1.5218e+00],\n",
       "         [-5.4511e-01, -5.2840e-01, -8.6018e-01, -1.6717e+00, -1.5686e+00],\n",
       "         [ 1.3005e-01,  1.5010e-01,  3.7940e-01, -1.2816e-01,  1.3125e+00],\n",
       "         [ 4.2240e-01,  4.6525e-01,  8.7129e-01, -3.8793e-01,  6.7237e-01],\n",
       "         [ 2.8134e-01,  3.1043e-01,  4.8433e-01, -4.2547e-01,  4.1743e-01],\n",
       "         [-3.8661e-02, -2.6836e-02,  5.4229e-01,  4.4888e-01,  2.9706e-01],\n",
       "         [-4.7331e-01, -4.9278e-01, -6.0126e-01,  2.2622e-02, -3.8108e-01],\n",
       "         [-1.8603e-01, -2.1009e-01, -1.3057e+00, -1.0421e+00, -1.5747e-01],\n",
       "         [ 4.2203e-02,  1.4131e-02, -5.1260e-01,  2.1157e-01,  4.9680e-01],\n",
       "         [-6.4977e-02, -1.0578e-01, -9.6086e-01, -3.4000e-02, -7.8452e-01],\n",
       "         [-3.1644e-01, -3.0070e-01,  2.8349e-01,  5.6111e-02, -9.0598e-02],\n",
       "         [ 4.9741e-01,  4.7166e-01,  1.5061e-01,  8.8860e-01,  7.2272e-01],\n",
       "         [ 2.8303e-02,  2.1088e-03, -5.3550e-01,  5.6242e-02, -5.9406e-01],\n",
       "         [ 4.1595e-01,  3.9781e-01, -2.4066e-01, -2.1034e-02, -9.0654e-01],\n",
       "         [ 4.8548e-01,  4.8657e-01,  3.4286e-01,  1.9569e-01, -1.1420e-01],\n",
       "         [ 4.9628e-01,  5.0495e-01,  1.3236e+00,  1.4242e+00,  3.9618e-02],\n",
       "         [ 1.6589e-01,  2.0465e-01,  1.0264e+00,  1.9634e-01,  1.6428e+00],\n",
       "         [ 2.7215e-02,  6.4328e-02,  7.6227e-01, -8.8521e-02,  1.2743e+00],\n",
       "         [ 5.5320e-01,  5.8348e-01,  7.3578e-01, -2.9485e-01, -5.0097e-01],\n",
       "         [ 4.6865e-01,  4.7829e-01,  5.8679e-01,  3.7045e-01,  1.5331e+00],\n",
       "         [ 4.2005e-01,  4.1996e-01,  1.1582e+00,  1.5281e+00,  9.1532e-02],\n",
       "         [-1.2967e-01, -1.0164e-01,  4.2293e-01, -2.6893e-01, -2.7198e-02],\n",
       "         [ 1.6653e-01,  1.4645e-01,  3.1838e-01,  1.0736e+00, -3.7515e-01],\n",
       "         [-1.8224e-02, -4.6720e-02,  2.7132e-02,  1.0710e+00,  2.8350e-01],\n",
       "         [ 7.3325e-02,  7.6706e-02, -5.9156e-01, -1.0697e+00, -1.3196e-01]]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_chomp1D = chomp1D(output_conv1, 2)\n",
    "print(output_chomp1D.shape)\n",
    "output_chomp1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e94529a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 160, 1])\n",
      "As we can see, each input single value turn into 160 values, and these are the important features of this single value. The length of the input series is disregarded, ie 5 becomes 1. So now the input series is represented by 1 number that has 160 features.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3231],\n",
       "         [ 0.5045],\n",
       "         [-0.4488],\n",
       "         [ 0.1002],\n",
       "         [ 1.2276],\n",
       "         [-0.3638],\n",
       "         [-0.3020],\n",
       "         [ 1.0079],\n",
       "         [ 0.1631],\n",
       "         [ 0.3172],\n",
       "         [ 0.8492],\n",
       "         [ 0.7530],\n",
       "         [ 0.3086],\n",
       "         [ 0.5838],\n",
       "         [ 0.5164],\n",
       "         [ 1.0759],\n",
       "         [ 1.1283],\n",
       "         [ 0.5069],\n",
       "         [ 0.5667],\n",
       "         [ 1.5377],\n",
       "         [-0.5124],\n",
       "         [ 0.3474],\n",
       "         [ 0.6573],\n",
       "         [ 0.9657],\n",
       "         [ 0.7789],\n",
       "         [ 0.6769],\n",
       "         [ 1.3309],\n",
       "         [ 0.1231],\n",
       "         [ 1.2434],\n",
       "         [ 0.3378],\n",
       "         [ 1.2515],\n",
       "         [ 1.4095],\n",
       "         [ 1.2538],\n",
       "         [ 1.1985],\n",
       "         [ 0.5482],\n",
       "         [-0.1737],\n",
       "         [ 0.3120],\n",
       "         [ 1.1358],\n",
       "         [ 0.4390],\n",
       "         [ 0.1965],\n",
       "         [ 1.0701],\n",
       "         [ 0.7319],\n",
       "         [ 0.0540],\n",
       "         [ 1.0481],\n",
       "         [ 0.0911],\n",
       "         [ 0.3387],\n",
       "         [ 0.3341],\n",
       "         [-0.2482],\n",
       "         [ 0.1200],\n",
       "         [ 0.6040],\n",
       "         [ 0.3575],\n",
       "         [ 0.6982],\n",
       "         [ 0.2079],\n",
       "         [ 0.4032],\n",
       "         [ 1.3808],\n",
       "         [ 0.3828],\n",
       "         [ 0.0902],\n",
       "         [ 1.4139],\n",
       "         [ 0.1298],\n",
       "         [ 0.8692],\n",
       "         [ 0.3930],\n",
       "         [ 1.2059],\n",
       "         [ 0.1555],\n",
       "         [-0.2337],\n",
       "         [ 0.4658],\n",
       "         [ 0.5107],\n",
       "         [-0.2310],\n",
       "         [-0.0237],\n",
       "         [-0.0630],\n",
       "         [-0.0879],\n",
       "         [-0.2717],\n",
       "         [ 0.4232],\n",
       "         [ 0.5686],\n",
       "         [ 1.3301],\n",
       "         [-0.1363],\n",
       "         [ 1.1226],\n",
       "         [ 0.3905],\n",
       "         [ 0.8360],\n",
       "         [ 0.6219],\n",
       "         [ 0.6279],\n",
       "         [ 0.5661],\n",
       "         [ 0.1570],\n",
       "         [ 0.0958],\n",
       "         [ 0.7620],\n",
       "         [ 1.3436],\n",
       "         [ 0.7149],\n",
       "         [ 1.4459],\n",
       "         [ 1.2893],\n",
       "         [ 0.4632],\n",
       "         [ 0.2799],\n",
       "         [-0.1325],\n",
       "         [ 1.3202],\n",
       "         [ 1.2172],\n",
       "         [ 0.8215],\n",
       "         [ 0.0357],\n",
       "         [ 1.2486],\n",
       "         [ 0.5523],\n",
       "         [ 0.5563],\n",
       "         [ 1.1180],\n",
       "         [ 0.6392],\n",
       "         [ 0.1919],\n",
       "         [ 0.1625],\n",
       "         [ 1.6262],\n",
       "         [ 0.6052],\n",
       "         [ 1.5951],\n",
       "         [-0.3358],\n",
       "         [ 1.3122],\n",
       "         [-0.0079],\n",
       "         [ 0.1420],\n",
       "         [ 0.7212],\n",
       "         [ 1.0007],\n",
       "         [-0.0266],\n",
       "         [ 0.8134],\n",
       "         [ 0.3289],\n",
       "         [ 0.7091],\n",
       "         [ 0.2966],\n",
       "         [ 0.4676],\n",
       "         [ 0.3678],\n",
       "         [ 1.0835],\n",
       "         [ 0.5861],\n",
       "         [-0.1999],\n",
       "         [ 0.5995],\n",
       "         [ 1.6760],\n",
       "         [ 0.7060],\n",
       "         [ 0.3760],\n",
       "         [ 1.4104],\n",
       "         [ 0.6486],\n",
       "         [ 1.0809],\n",
       "         [ 0.0185],\n",
       "         [ 0.9608],\n",
       "         [-0.1369],\n",
       "         [-0.1836],\n",
       "         [ 0.5951],\n",
       "         [ 1.4185],\n",
       "         [-0.0814],\n",
       "         [ 1.5218],\n",
       "         [-0.5284],\n",
       "         [ 1.3125],\n",
       "         [ 0.8713],\n",
       "         [ 0.4843],\n",
       "         [ 0.5423],\n",
       "         [ 0.0226],\n",
       "         [-0.1575],\n",
       "         [ 0.4968],\n",
       "         [-0.0340],\n",
       "         [ 0.2835],\n",
       "         [ 0.8886],\n",
       "         [ 0.0562],\n",
       "         [ 0.4159],\n",
       "         [ 0.4866],\n",
       "         [ 1.4242],\n",
       "         [ 1.6428],\n",
       "         [ 1.2743],\n",
       "         [ 0.7358],\n",
       "         [ 1.5331],\n",
       "         [ 1.5281],\n",
       "         [ 0.4229],\n",
       "         [ 1.0736],\n",
       "         [ 1.0710],\n",
       "         [ 0.0767]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_size = torch.nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "output_maxpool = reduce_size(output_chomp1D)\n",
    "\n",
    "print(output_maxpool.shape)\n",
    "print(\"As we can see, each input single value turn into 160 values, and these are the important features of this single value. The length of the input series is disregarded, ie 5 becomes 1. So now the input series is represented by 1 number that has 160 features.\")\n",
    "output_maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de478a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 160])\n",
      "For a single value of the input series, we don't need one bracket for each learnt feature, so we squeeze out the bracket. Now the features of a single input value becomes one vector.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3231,  0.5045, -0.4488,  0.1002,  1.2276, -0.3638, -0.3020,  1.0079,\n",
       "          0.1631,  0.3172,  0.8492,  0.7530,  0.3086,  0.5838,  0.5164,  1.0759,\n",
       "          1.1283,  0.5069,  0.5667,  1.5377, -0.5124,  0.3474,  0.6573,  0.9657,\n",
       "          0.7789,  0.6769,  1.3309,  0.1231,  1.2434,  0.3378,  1.2515,  1.4095,\n",
       "          1.2538,  1.1985,  0.5482, -0.1737,  0.3120,  1.1358,  0.4390,  0.1965,\n",
       "          1.0701,  0.7319,  0.0540,  1.0481,  0.0911,  0.3387,  0.3341, -0.2482,\n",
       "          0.1200,  0.6040,  0.3575,  0.6982,  0.2079,  0.4032,  1.3808,  0.3828,\n",
       "          0.0902,  1.4139,  0.1298,  0.8692,  0.3930,  1.2059,  0.1555, -0.2337,\n",
       "          0.4658,  0.5107, -0.2310, -0.0237, -0.0630, -0.0879, -0.2717,  0.4232,\n",
       "          0.5686,  1.3301, -0.1363,  1.1226,  0.3905,  0.8360,  0.6219,  0.6279,\n",
       "          0.5661,  0.1570,  0.0958,  0.7620,  1.3436,  0.7149,  1.4459,  1.2893,\n",
       "          0.4632,  0.2799, -0.1325,  1.3202,  1.2172,  0.8215,  0.0357,  1.2486,\n",
       "          0.5523,  0.5563,  1.1180,  0.6392,  0.1919,  0.1625,  1.6262,  0.6052,\n",
       "          1.5951, -0.3358,  1.3122, -0.0079,  0.1420,  0.7212,  1.0007, -0.0266,\n",
       "          0.8134,  0.3289,  0.7091,  0.2966,  0.4676,  0.3678,  1.0835,  0.5861,\n",
       "         -0.1999,  0.5995,  1.6760,  0.7060,  0.3760,  1.4104,  0.6486,  1.0809,\n",
       "          0.0185,  0.9608, -0.1369, -0.1836,  0.5951,  1.4185, -0.0814,  1.5218,\n",
       "         -0.5284,  1.3125,  0.8713,  0.4843,  0.5423,  0.0226, -0.1575,  0.4968,\n",
       "         -0.0340,  0.2835,  0.8886,  0.0562,  0.4159,  0.4866,  1.4242,  1.6428,\n",
       "          1.2743,  0.7358,  1.5331,  1.5281,  0.4229,  1.0736,  1.0710,  0.0767]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_squeeze = output_maxpool.squeeze(2)\n",
    "print(output_squeeze.shape)\n",
    "print(\"For a single value of the input series, we don't need one bracket for each learnt feature, so we squeeze out the bracket. Now the features of a single input value becomes one vector.\")\n",
    "output_squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07b5a89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 320])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1423e+00, -1.8943e-01,  3.9639e-01,  3.3756e-01, -2.9349e-01,\n",
       "         -4.1077e-01, -1.1545e+00,  1.6256e-01,  3.2954e-02,  3.2515e-01,\n",
       "          1.9370e-01, -6.0564e-01,  3.9789e-02,  4.1061e-01,  4.6652e-01,\n",
       "         -5.1951e-01,  4.5546e-02,  3.1762e-01, -1.7151e-01,  4.1700e-01,\n",
       "         -3.3019e-01, -8.7405e-01, -6.5281e-01, -7.2817e-02,  7.8189e-01,\n",
       "          2.9201e-01, -7.5600e-02,  2.6827e-01,  3.6822e-03,  3.3139e-01,\n",
       "         -6.9273e-01,  5.6664e-01, -2.7964e-02, -4.6031e-02,  2.6289e-01,\n",
       "         -3.8485e-01,  4.3438e-01, -9.6048e-02, -1.4830e-02, -2.8898e-01,\n",
       "         -3.3543e-01, -6.8962e-02, -8.0126e-02, -6.9782e-01,  2.0653e-01,\n",
       "          4.6897e-01,  4.4699e-01, -1.0856e-01, -5.0332e-01,  3.3947e-01,\n",
       "         -5.9353e-01, -4.3386e-01,  3.8003e-01,  7.2433e-01,  3.3862e-01,\n",
       "         -1.9343e-01, -3.1538e-01, -1.6528e-01, -1.6477e-01,  8.6288e-01,\n",
       "         -5.8635e-02,  4.1432e-01, -5.0810e-01, -7.2204e-02,  3.4944e-01,\n",
       "          1.9094e-01,  2.1328e-02,  3.4937e-01, -2.9062e-01,  7.6983e-01,\n",
       "         -4.8359e-01, -2.0729e-01, -9.1935e-01, -2.0777e-01, -5.4449e-01,\n",
       "         -2.2195e-02,  6.1002e-01, -2.8114e-01, -2.0785e-01, -4.4106e-01,\n",
       "          2.8061e-01,  4.6243e-01,  7.4922e-02, -8.9896e-01, -6.4174e-01,\n",
       "         -5.6579e-01,  1.5774e-01, -5.9885e-01,  5.0316e-01,  2.8362e-01,\n",
       "          2.3433e-01, -3.3243e-01,  1.2845e-01, -8.3793e-02,  1.7721e-01,\n",
       "         -3.4918e-01, -5.8407e-01, -3.6118e-01, -7.5889e-01, -1.7489e-01,\n",
       "         -6.0389e-01,  3.0745e-01, -2.8460e-01, -2.6614e-01,  8.4578e-02,\n",
       "          8.2207e-01, -2.5790e-01, -3.4758e-01,  3.9468e-01,  1.4504e-01,\n",
       "         -6.0645e-01,  6.3348e-01,  2.6332e-01, -4.0152e-01, -2.8810e-01,\n",
       "         -2.5713e-01,  9.4178e-01, -1.0922e-01,  1.6111e-01, -3.3090e-02,\n",
       "         -5.1190e-01,  4.8761e-02, -2.9158e-01,  5.6592e-01, -3.8270e-01,\n",
       "          3.2896e-01, -2.4534e-02, -1.2588e-01, -1.0159e-01, -2.8373e-01,\n",
       "         -3.2913e-01,  6.1811e-01,  1.2653e-01, -1.8292e-01,  2.4551e-01,\n",
       "          3.0944e-01,  3.0043e-01,  2.2943e-01, -1.8478e-01, -2.3097e-01,\n",
       "         -3.2972e-01, -9.4949e-02, -3.6173e-02, -2.2373e-01,  1.7757e-01,\n",
       "          2.2334e-01,  9.0453e-02,  5.9848e-01,  5.5757e-02,  2.1086e-01,\n",
       "         -6.7813e-01,  1.1172e-03, -3.4094e-01, -3.7269e-01,  7.1071e-01,\n",
       "          6.0048e-02,  2.8268e-01, -3.0626e-01, -6.1003e-01,  1.9090e-01,\n",
       "         -5.3851e-01,  1.2636e+00, -1.3995e-02, -6.0938e-01,  2.7554e-01,\n",
       "          1.6124e-01,  4.7242e-01,  3.6081e-01,  2.1278e-02, -5.5060e-01,\n",
       "         -4.9096e-01,  3.1298e-01,  8.0499e-02, -1.0410e+00,  8.7345e-02,\n",
       "         -3.2229e-01,  1.5863e-01, -3.4131e-01,  6.4052e-01, -4.0530e-01,\n",
       "         -5.0285e-01, -2.2243e-01, -1.1232e-01, -4.3356e-01,  2.6983e-01,\n",
       "         -8.6259e-01, -1.3875e-01, -4.5629e-01, -6.1190e-01, -6.4761e-01,\n",
       "          2.7162e-01, -9.3046e-01, -3.2030e-02, -1.4970e-01,  4.5524e-01,\n",
       "         -6.8747e-01,  6.7696e-01, -6.0607e-01, -3.7694e-02, -1.4402e-02,\n",
       "         -2.2775e-01,  4.1548e-01,  5.5064e-01,  2.3558e-01, -1.3481e-01,\n",
       "          7.7193e-01,  2.3965e-01, -2.7946e-01,  1.0044e-01,  2.8470e-02,\n",
       "         -1.7992e-01, -4.0064e-01,  4.9897e-01,  9.1775e-01,  6.4151e-02,\n",
       "          6.5737e-01,  3.9880e-01,  5.4390e-01,  2.8143e-01,  4.7487e-02,\n",
       "          1.3099e-01,  2.0124e-01, -5.2368e-03, -7.5782e-01,  6.3654e-01,\n",
       "          2.6111e-01,  4.4557e-01, -6.6053e-01,  1.4079e-03, -1.6726e-01,\n",
       "          4.4176e-02, -1.7198e-01,  6.6394e-01, -3.3351e-01,  2.8065e-01,\n",
       "         -5.3078e-01, -4.2918e-01, -2.2969e-02, -6.9426e-01,  1.4872e+00,\n",
       "          2.3283e-01,  9.5647e-01, -5.4518e-01, -5.0249e-01,  7.5259e-01,\n",
       "          7.3968e-01, -4.6914e-01, -4.6921e-01, -6.2577e-03, -7.3330e-01,\n",
       "          1.4541e-01,  1.6931e-01,  3.5228e-01, -1.5668e-01, -1.0629e-01,\n",
       "         -1.2831e+00, -5.4885e-02,  8.3479e-01, -1.7797e-01,  2.7411e-01,\n",
       "         -1.6134e-01,  1.6608e-01,  3.9020e-01,  6.3197e-01, -6.4904e-01,\n",
       "         -8.1212e-01,  3.9032e-01,  2.5710e-01,  4.1114e-01, -7.2231e-01,\n",
       "         -7.9200e-01, -1.0066e+00, -1.0049e-01,  6.9036e-01, -2.1964e-01,\n",
       "          1.6444e-01,  3.6110e-01,  9.5864e-01,  3.9797e-01, -6.4071e-01,\n",
       "          9.6294e-02,  4.0718e-01, -1.4222e-01,  9.4959e-01, -3.5169e-01,\n",
       "          3.0530e-01, -3.3794e-01,  2.4018e-01,  2.9156e-01, -4.3596e-01,\n",
       "          4.3623e-01, -8.3233e-02,  1.4609e-01, -3.5063e-01, -7.5010e-01,\n",
       "         -2.6203e-01, -2.5089e-01, -4.1955e-01, -4.0003e-01, -5.7840e-02,\n",
       "          2.1154e-01, -1.0803e-01,  1.2085e+00,  8.1527e-01,  1.4967e-01,\n",
       "         -3.7628e-01, -2.1177e-01, -3.8946e-01,  5.6881e-01, -6.5135e-01,\n",
       "          4.6887e-01,  1.0779e-01, -5.3028e-01,  4.7232e-01,  8.9048e-01,\n",
       "         -1.0526e+00,  6.9055e-01, -3.3288e-01, -2.1823e-01, -4.9731e-01]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# user-defined parameter which is the length they want their latent representation to be\n",
    "reduced_size = 160\n",
    "out_channels = 320\n",
    "\n",
    "linear = torch.nn.Linear(in_features = reduced_size, out_features = out_channels)\n",
    "output_linear = linear(output_squeeze)\n",
    "\n",
    "print(output_linear.shape)\n",
    "output_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8f6ef8",
   "metadata": {},
   "source": [
    "# When there is NaN value in the input series\n",
    "AdaptiveMaxPool1d()will just return NaN for all inputs. <br>\n",
    "This behavior is discussed and coded via:https://github.com/pytorch/pytorch/issues/7645"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1556eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are several Conv1 layers in the whole network. All of the conv1's out_channels=40 except the last conv1's out_channel=160.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Conv1d(1, 160, kernel_size=(3,), stride=(1,), padding=(2,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"There are several Conv1 layers in the whole network. All of the conv1's out_channels=40 except the last conv1's out_channel=160.\")\n",
    "conv1 = torch.nn.Conv1d(in_channels=1, out_channels=160, kernel_size=3, padding=2, dilation=1)\n",
    "conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f34ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chomp1D(x, chomp_size):\n",
    "    return x[:, :, :-chomp_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "138c5da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assume we input a time series, [batchSize= 1, column number = 1, length of the series = 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6887, -0.0834, -0.7951, -0.9995,  1.3388, -0.6010,  0.5827,\n",
       "          -0.3108, -0.4429,  0.0976]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_withNaN = torch.randn(1, 1, 10)\n",
    "print(\"Assume we input a time series, [batchSize= 1, column number = 1, length of the series = 10]\")\n",
    "x_withNaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9db2cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6887,     nan, -0.7951, -0.9995,  1.3388, -0.6010,  0.5827,\n",
       "          -0.3108, -0.4429,  0.0976]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the 2nd value to be NaN\n",
    "x_withNaN[0,0,1] = torch.log(torch.tensor([-1.]))\n",
    "x_withNaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce88674f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 160, 12])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7459,     nan,     nan,  ..., -0.7167, -0.4518, -0.5824],\n",
       "         [-0.2732,     nan,     nan,  ...,  0.0880, -0.1842,  0.1432],\n",
       "         [ 0.3341,     nan,     nan,  ...,  0.3934,  0.5066,  0.4486],\n",
       "         ...,\n",
       "         [ 0.5083,     nan,     nan,  ...,  0.3397,  0.4813,  0.5202],\n",
       "         [-0.2204,     nan,     nan,  ..., -0.0588, -0.3568, -0.1415],\n",
       "         [ 0.6168,     nan,     nan,  ...,  0.6892,  0.3781,  0.5815]]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_conv1 = conv1(x_withNaN)\n",
    "print(output_conv1.shape)\n",
    "output_conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fb25d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 160, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7459,     nan,     nan,  ..., -0.2658, -0.9259, -0.7167],\n",
       "         [-0.2732,     nan,     nan,  ..., -0.5679,  0.2651,  0.0880],\n",
       "         [ 0.3341,     nan,     nan,  ...,  0.5717,  0.2685,  0.3934],\n",
       "         ...,\n",
       "         [ 0.5083,     nan,     nan,  ...,  0.6035,  0.4901,  0.3397],\n",
       "         [-0.2204,     nan,     nan,  ..., -0.6524,  0.1239, -0.0588],\n",
       "         [ 0.6168,     nan,     nan,  ...,  0.1080,  0.9212,  0.6892]]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_chomp1D = chomp1D(output_conv1, 2)\n",
    "print(output_chomp1D.shape)\n",
    "output_chomp1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b003dde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 160, 1])\n",
      "As we can see, each input single value turn into 160 values, and these are the important features of this single value. The length of the input series is disregarded, ie 10 becomes 1. So now the input series is represented by 1 number that has 160 features.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan],\n",
       "         [nan]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_size = torch.nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "output_maxpool = reduce_size(output_chomp1D)\n",
    "\n",
    "print(output_maxpool.shape)\n",
    "print(\"As we can see, each input single value turn into 160 values, and these are the important features of this single value. The length of the input series is disregarded, ie 10 becomes 1. So now the input series is represented by 1 number that has 160 features.\")\n",
    "output_maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bf0e3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 160])\n",
      "For a single value of the input series, we don't need one bracket for each learnt feature, so we squeeze out the bracket. Now the features of a single input value becomes one vector.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_squeeze = output_maxpool.squeeze(2)\n",
    "print(output_squeeze.shape)\n",
    "print(\"For a single value of the input series, we don't need one bracket for each learnt feature, so we squeeze out the bracket. Now the features of a single input value becomes one vector.\")\n",
    "output_squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b908a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 320])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# user-defined parameter which is the length they want their latent representation to be\n",
    "reduced_size = 160\n",
    "out_channels=320\n",
    "\n",
    "# Fracheschi messed up the order of in_features and out_features. I switched to correct one here.\n",
    "linear = torch.nn.Linear(in_features = reduced_size, out_features = out_channels)\n",
    "output_linear = linear(output_squeeze)\n",
    "\n",
    "print(output_linear.shape)\n",
    "output_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c20c83",
   "metadata": {},
   "source": [
    "# Unequal lengths vs Same length training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f86a13",
   "metadata": {},
   "source": [
    "## Same length example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4349c7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assume we input a time series, [batchSize= 3, column number = 2, length of the series = 4]\n",
      "As we can see, for each training set, the corresponding series all have same lengths. ie. all col1 has length 2, all col2 has length 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8408, -0.6546,     nan,     nan],\n",
       "         [-0.7015, -1.3145,  0.2572, -1.0638]],\n",
       "\n",
       "        [[-1.1626,  0.5573,     nan,     nan],\n",
       "         [ 2.6743,  1.3395, -0.2082,  0.8811]],\n",
       "\n",
       "        [[-0.4517, -0.1752,     nan,     nan],\n",
       "         [ 0.2501, -0.5502,  1.2579,  1.0505]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 2, 4)\n",
    "x[:, 0, 2:]=numpy.nan\n",
    "print(\"Assume we input a time series, [batchSize= 3, column number = 2, length of the series = 4]\")\n",
    "print(\"As we can see, for each training set, the corresponding series all have same lengths. ie. all col1 has length 2, all col2 has length 4\")\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646ef1d7",
   "metadata": {},
   "source": [
    "## Unequal lengths example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "021fe6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assume we input a time series, [batchSize= 3, column number = 2, length of the series = 4]\n",
      "The corresponding columns of training sets do not have the same length\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5961, -0.2652,  0.7300,     nan],\n",
       "         [ 0.9152,  0.9476, -0.1323, -1.3774]],\n",
       "\n",
       "        [[ 0.5036,  0.1680, -0.5816, -0.7644],\n",
       "         [-0.5558,  0.3305,     nan,     nan]],\n",
       "\n",
       "        [[ 0.6284, -0.0095,  0.4261,  0.2715],\n",
       "         [-0.5495, -0.6978, -0.0104, -0.2112]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 2, 4)\n",
    "x[0, 0, 3]=numpy.nan\n",
    "x[1, 1, 2:]=numpy.nan\n",
    "print(\"Assume we input a time series, [batchSize= 3, column number = 2, length of the series = 4]\")\n",
    "print(\"The corresponding columns of training sets do not have the same length\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97330077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scikit_wrappers.fit_encoder(X) will detect we have unequal lengths time series\n",
    "varying = bool(numpy.isnan(numpy.sum(x.numpy())))\n",
    "varying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a32a1dc",
   "metadata": {},
   "source": [
    "Then `scikit_wrappers.fit_encoder(X`) will set:<br>\n",
    "`loss = loss_varying( batch, self.encoder, train, save_memory=save_memory )`<br>\n",
    "where `loss_varying = triplet_loss.TripletLossVaryingLength(compared_length, nb_random_samples, negative_penalty)`<br>\n",
    "The need to distinguish between varying length from same length training sets is because we need to choose samples that are within series length (NaN not included), and when lengths are varying, we need to choose different lengths accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bb0a2f",
   "metadata": {},
   "source": [
    "# Challenges \n",
    "1. Preserves only temporal order of values but not exact time of a value.\n",
    "2. Cannot handle missing value in between the time series e.g. [2, 4, 5, NaN, 8, 0]. Only accepts time series where missing values are at begining or end.\n",
    "3. Due to (1) and (2) above, it assumes equidistant time between values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df55173",
   "metadata": {},
   "source": [
    "## Solution 1 - keep embedding method, change anomaly detection method\n",
    "Use this embedding method and disregard exact time information, and a different anomaly detection method is needed.\n",
    "#### Modify current anomaly detection algorithm:\n",
    "Instead of using time interval to decide whether a value belongs or not, we can use a window of neighbor count.\n",
    "#### Other anomaly algorithms:\n",
    "- One-Class Support Vector Machine (Trained on normal data)\n",
    "- Outlier detection with Local Outlier Factor (Computes local density of data)\n",
    "- Kernel Density Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104e4b59",
   "metadata": {},
   "source": [
    "## Solution 2 - change embedding method\n",
    "1. Use call gaps to engineer a dataset only contains 'normal' sensor data.\n",
    "2. Detect anomalies using method in paper \"Learning Neural Representations for Network Anomaly Detection\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0372b16d",
   "metadata": {},
   "source": [
    "# Train the model with CPU (cuz I don't have Cuda)\n",
    "Takes forever, and I am not exaggerating on the word \"forever\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea0aa46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# sys.path.append(os.getcwd())\n",
    "# sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf738f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Paws\\\\Desktop\\\\MasterProject\\\\Franceschi'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check current working directory, make we are in the Fraceschi folder.\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "869ea085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paws\\Desktop\\MasterProject\\Franceschi\n"
     ]
    }
   ],
   "source": [
    "# If we aren't in Franceschi folder, use % cd to navigate to that folder.\n",
    "%cd C:\\\\Users\\\\Paws\\\\Desktop\\\\MasterProject\\\\Franceschi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bdc1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training!\n",
    "!python ucr.py --dataset Mallat --path C:\\\\Users\\\\Paws\\\\Desktop\\\\MasterProject\\\\ExtraDatasets\\\\UCRArchive_2018 --save_path C:\\\\Users\\\\Paws\\\\Desktop\\\\MasterProject\\\\Franceschi\\\\TrainedModel --hyper default_hyperparameters.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
